#!/usr/bin/env Rscript

###############################################################################
## Project ID: GB-LZ-1373
## Authors: Reuben Thomas, Natalie Elphick, Ayushi Agrawal
##
## Script Goal: Optimize resolution parameter; check silhouette score
## distribution and decide the optimal resolution
##
## Usage example:
## Rscript 09_03_cluster_resolution_optimization.R \
##  --input 'input_seurat_object.RDS' \  # Seurat object with multiple samples
##  --output '/output_directory' \       # Location for output files
##  --mode "scRNA" \                     # Is the input scRNA or CyTOF?
##  --output_prefix "outputs_scRNA_" \   # Prefix for output files
##  --ndim 20 \                          # Number of PCs to use
##  --metadata "sample"                  # Name of sample identifier
##  --cluster_subset "2,3"               # Clusters to subset and optimize for
##  --cores 4                            # Number of cores to use
##
## Run "Rscript 09_03_cluster_resolution_optimization.R --help" for more information
###############################################################################


# Get input arguments -----------------------------------------------------
library(optparse)
option_list <- list(
  make_option(c("-i", "--input"),
              action = "store", default = NA, type = "character",
              help = "Input Seurat object in RDS format (required)"
  ),
  make_option(c("-o", "--output"),
              action = "store", default = NA, type = "character",
              help = "Output directory, will create if it doesn't exist (required)"
  ),
  make_option(c("-c", "--cores"),
              action = "store", default = 2, type = "numeric",
              help = "Number of cores to use, [default %default]"
  ),
  make_option(c("--mode"),
              action = "store", default = NA, type = "character",
              help = "Input data type vaid options are 'scRNA' or 'CyTOF' (required)"
  ),
  make_option(c("--output_prefix"),
              action = "store",
              default = "optimize_resolution",
              type = "character",
              help = "Prefix for output files, [default %default]"
  ),
  make_option(c("-n", "--ndim"),
              action = "store", default = NA, type = "numeric",
              help = "Number of principal components to use (required)"
  ),
  make_option(c("-m", "--metadata"),
              action = "store", default = NA, type = "character",
              help = "The metadata value for sample in the Seurat object (required)",
  ),
  make_option(c("--cluster_subset"),
              action = "store", default = NA, type = "character",
              help = "If provided, the input will be subset by seurat clusters to include provided clusters"
  ),
  make_option("--within_batch_var",
              action = "store", default = NA, type = "character",
              help = "The metadata value for a batching variable that if provided, 
    will be used to train the model on samples within the same batch. Only works if samples are not spread
    across batches (optional)"
  )
)

opt <- parse_args(OptionParser(option_list = option_list))

# Check if required args are provided
if (is.na(opt$metadata) | is.na(opt$input) | is.na(opt$output) | is.na(opt$ndim) | is.na(opt$mode)) {
  stop("Missing one or more required arguments")
}

# Check if mode argument is a valid option
if (opt$mode != "scRNA" & opt$mode != "CyTOF") {
  stop("Invalid option for --mode, must be scRNA or CyTOF")
}




# Load required packages --------------------------------------------------
library(tidyverse)
library(Seurat)
library(ranger)
library(cluster)
library(foreach)
library(doParallel)
library(future)


# Prevent the Rplots.pdf from being autogenerated
pdf(NULL)

# Leave one core free
nCores <- opt$cores - 1

# Register the cluster for foreach
registerDoParallel(cores = nCores)
getDoParWorkers()
getDoParName()

set.seed(332)
options(warn = 1)
options(future.globals.maxSize = 8000 * 1024^2)



# create the results folders and all parent folders if they don't exist
if (!(dir.exists(opt$output))) {
  dir.create(opt$output, recursive = TRUE)
}


# Initialize variables & read in data -------------------------------------

# Read in the Seurat object
input_rds <- readRDS(opt$input)

# If the cluster_subset argument is provided, subset the Seurat object to 
# include only the seurat clusters provided
if (!is.na(opt$cluster_subset)) {
  temp <- strsplit(as.character(opt$cluster_subset), ',')
  cell_type_clusters <- as.numeric(as.character(unlist(temp)))
  print(cell_type_clusters)
  temp_rds <- subset(input_rds, idents = cell_type_clusters)
  temp_rds$seurat_clusters <- droplevels(temp_rds$seurat_clusters)
  input_rds <- temp_rds
  print(input_rds)
  print(table(input_rds$seurat_clusters))
}

# Set number of PCs to use
ndim <- opt$ndim

# Set range of resolutions to test
res_range <- c(seq(0.02,0.08,0.02), seq(0.1,1.2,0.1))

# Get sample information
sample_obj_id <- opt$metadata
n_samples <- length(unique(input_rds@meta.data[[sample_obj_id]]))
sample_names <- as.vector((unique(input_rds@meta.data[[sample_obj_id]])))




# Main function -----------------------------------------------------------

# CyTOF main function
run_rf_cytof <- function(sample, res) {
  this_sample <- sample_names[sample]
  print(paste0("Working on: ", this_sample, "  Resolution: ", res))
  
  # Extract the data and reformat for random forest
  df <- GetAssayData(input_rds, slot = "counts", assay = "RNA") %>%
    as.data.frame() %>%
    t() %>%
    as_tibble(rownames = "CellID") %>%
    mutate(SampleID = input_rds@meta.data[[sample_obj_id]]) %>%
    select(CellID, SampleID, everything())
  
  # Fix illegal column names
  names(df) <- gsub("-", "_", names(df), fixed = TRUE)
  
  # Subset the Seurat object to cluster the training samples only
  train_cells <- colnames(input_rds)[which(input_rds[[]][opt$metadata] != this_sample)]
  train_seurat <- subset(input_rds, cells = train_cells)
  
  # Subset the training and testing samples
  train_df <- df %>% filter(SampleID != this_sample)
  test_df <- df %>% filter(SampleID == this_sample)
  
  # Cluster the training data
  train_seurat <- ScaleData(train_seurat, features = NULL)
  train_seurat <- FindVariableFeatures(train_seurat,
                                       selection.method = "vst", nfeatures = ndim
  )
  train_seurat <- RunPCA(train_seurat, npcs = ndim, approx = FALSE)
  train_seurat <- FindNeighbors(object = train_seurat, dims = 1:ndim)
  
  train_seurat <- FindClusters(object = train_seurat, resolution = res)
  
  # Label clusters in the training data
  train_df <- train_df %>%
    mutate(clusters = train_seurat@meta.data$seurat_clusters) %>%
    as.data.frame()
  
  # Train the model
  rf <- ranger(as.factor(clusters) ~ .,
               data = train_df[, 3:ncol(train_df)],
               num.trees = 1000,
               write.forest = TRUE,
               num.threads = 1
  )
  
  # Predict on the hold out sample
  predicted <- predict(rf, test_df[, 3:ncol(test_df)])
  predicted <- predictions(predicted)
  predicted_clusters_table <- table(predicted)
  
  trained_clusters_table <- table(train_seurat@meta.data$seurat_clusters)
  
  # Get silhouette scores
  sil <- silhouette(
    as.numeric(as.character(predicted)),
    dist(test_df[, 3:ncol(test_df)])
  )
  # Set values to NA if there is only one cluster
  if (class(sil) == "logical") {
    sil_mean <- NA
    sil_group_mean <- NA
  } else {
    sil_summary <- summary(sil)
    sil_mean <- sil_summary$avg.width
    sil_group_mean <- mean(sil_summary$clus.avg.widths)
  }
  
  tibble(
    resolution = res,
    test_sample = this_sample,
    avg_width = sil_mean,
    cluster_avg_widths = sil_group_mean,
    n_predicted_clusters = length(unique(as.character(predicted))),
    min_predicted_cell_per_cluster = min(predicted_clusters_table),
    max_predicted_cell_per_cluster = max(predicted_clusters_table),
    n_trained_clusters = length(trained_clusters_table),
    min_trained_cell_per_cluster = min(trained_clusters_table),
    max_trained_cell_per_cluster = max(trained_clusters_table)
  )
}

# scRNA main function
run_rf_scrna <- function(res, train_seurat, test_seurat) {
  print(paste0("Working on resolution: ", res))
  
  # Cluster training samples
  train_seurat <- FindClusters(
    object = train_seurat,
    resolution = res,
    verbose = FALSE
  )
  
  # Features present in both the training variable features and test sample
  common_features <- intersect(
    rownames(test_seurat@assays[["SCT"]]@scale.data),
    VariableFeatures(train_seurat)
  )
  # Extract the loadings for common features from the training data
  loadings_common_features <- Loadings(train_seurat[["pca"]]) %>%
    as_tibble(rownames = "Features") %>%
    filter(Features %in% common_features) %>%
    as.matrix()
  
  rownames(loadings_common_features) <- loadings_common_features[, 1]
  loadings_common_features <- loadings_common_features[, -1]
  class(loadings_common_features) <- "numeric"
  
  pca_train_data <- as.matrix(train_seurat[["SCT"]]@scale.data)[common_features, ] %>%
    t() %*% loadings_common_features
  
  
  # Project the cells in the test data onto the PCs
  # identified using the training data
  pca_test_data <- as.matrix(test_seurat[["SCT"]]@scale.data)[common_features, ] %>%
    t() %*% loadings_common_features
  # Free up some memory
  rm(test_seurat)
  
  # Number of dimensions to train with
  train_df <- pca_train_data[, 1:ndim]
  test_df <- pca_test_data[, 1:ndim]
  
  # Label clusters in the training data
  train_df <- train_df %>%
    as.data.frame() %>%
    mutate(clusters = train_seurat@meta.data$seurat_clusters)
  # Get the cluster info for the training data
  trained_clusters_table <- table(train_seurat@meta.data$seurat_clusters)
  
  rm(train_seurat)
  
  # Train the model
  rf <- ranger(as.factor(clusters) ~ .,
               data = train_df,
               num.trees = 1000,
               write.forest = TRUE,
               num.threads = 1
  )
  
  # Predict on the hold out sample
  predicted <- predict(rf, test_df)
  predicted <- predictions(predicted)
  predicted_clusters_table <- table(predicted)
  rm(rf)
  
  # Get silhouette scores
  sil <- silhouette(
    as.numeric(as.character(predicted)),
    dist(test_df)
  )
  # Set values to NA if there is only one cluster
  if (class(sil) == "logical") {
    sil_mean <- NA
    sil_group_mean <- NA
  } else {
    sil_summary <- summary(sil)
    sil_mean <- sil_summary$avg.width
    sil_group_mean <- mean(sil_summary$clus.avg.widths)
  }
  
  
  tibble(
    resolution = res,
    test_sample = this_sample,
    avg_width = sil_mean,
    cluster_avg_widths = sil_group_mean,
    n_predicted_clusters = length(unique(as.character(predicted))),
    min_predicted_cell_per_cluster = min(predicted_clusters_table),
    max_predicted_cell_per_cluster = max(predicted_clusters_table),
    n_trained_clusters = length(trained_clusters_table),
    min_trained_cell_per_cluster = min(trained_clusters_table),
    max_trained_cell_per_cluster = max(trained_clusters_table)
  )
  
}


if (opt$mode == "scRNA") {
  res_tbl <- NULL
  for (sam in seq(1:n_samples)) {
    this_sample <- sample_names[sam]
    print(paste0("Working on: ", this_sample))
    # If within_batch_var is provided, then use only traning samples from the same batch
    if (!is.na(opt$within_batch_var)) {
      # Get the batch of this_sample
      this_batch <- input_rds@meta.data %>%
        filter(get(sample_obj_id) == this_sample) %>%
        pull(get(opt$within_batch_var)) %>%
        unique()
      
      
      if (length(this_batch) > 1) {
        stop("More than one batch found for this sample")
      }
      train_cells <- input_rds@meta.data %>%
        filter(get(sample_obj_id) != this_sample & get(opt$within_batch_var) == this_batch) %>%
        rownames()
      train_seurat <- subset(input_rds, cells = train_cells)
      print(paste0("Using only samples from batch: ", unique(train_seurat@meta.data[[opt$within_batch_var]])))
      # Subset the test sample
      test_cells <- colnames(input_rds)[which(input_rds[[]][sample_obj_id] == this_sample)]
      test_seurat <- subset(input_rds, cells = test_cells)
    } else {
      
      # Subset the Seurat object to cluster the training samples only
      train_cells <- colnames(input_rds)[which(input_rds[[]][sample_obj_id] != this_sample)]
      train_seurat <- subset(input_rds, cells = train_cells)
      # Subset the test sample
      test_cells <- colnames(input_rds)[which(input_rds[[]][sample_obj_id] == this_sample)]
      test_seurat <- subset(input_rds, cells = test_cells)
    }
    
    # Prep the Seurat objects
    train_seurat <- SCTransform(train_seurat,
                                vst.flavor = "v2",
                                verbose = FALSE
    )
    
    test_seurat <- SCTransform(test_seurat,
                               vst.flavor = "v2",
                               verbose = FALSE,
                               variable.features.n = length(rownames(test_seurat)),
                               return.only.var.genes = FALSE,
                               min_cells = 1
    )
    
    print(paste0(
      "Found ",
      length(intersect(
        rownames(test_seurat@assays[["SCT"]]@scale.data),
        VariableFeatures(train_seurat)
      )),
      " shared genes between testing and training data"
    ))
    
    train_seurat <- RunPCA(train_seurat,
                           npcs = ndim,
                           verbose = FALSE,
                           assay = "SCT"
    )
    
    train_seurat <- FindNeighbors(train_seurat,
                                  reduction = "pca",
                                  dims = 1:ndim,
                                  verbose = FALSE
    )
    
    
    this_res <- foreach(res = res_range, .combine = "rbind") %dopar% {
      run_rf_scrna(
        res = res,
        train_seurat = train_seurat,
        test_seurat = test_seurat
      )
    }
    res_tbl <- rbind(res_tbl, this_res)
  }
} else {
  res_tbl <- foreach(
    sample = seq(1:n_samples),
    .combine = "rbind"
  ) %:%
    foreach(res = res_range, .combine = "rbind") %dopar% {
      run_rf_cytof(sample = sample, res = res)
    }
}



# Summarize results
results_summary <- res_tbl %>%
  group_by(resolution) %>%
  summarize(
    mean_score = mean(avg_width, na.rm = TRUE),
    variance_score = var(avg_width, na.rm = TRUE),
    standard_error_score = sd(avg_width, na.rm = TRUE) / sqrt(length(avg_width)),
    cluster_mean_score = mean(cluster_avg_widths, na.rm = TRUE),
    cluster_variance_score = var(cluster_avg_widths, na.rm = TRUE),
    cluster_standard_error_score = sd(cluster_avg_widths,na.rm = TRUE) / sqrt(length(cluster_avg_widths))
  )

# Generate outputs --------------------------------------------------------

write_csv(
  x = res_tbl,
  file = paste0(
    opt$output,
    "/",
    opt$output_prefix,
    "_results_full.csv"
  )
)

write_csv(
  x = results_summary,
  file = paste0(
    opt$output,
    "/",
    opt$output_prefix,
    "_results_summary.csv"
  )
)

res_tbl %>%
  drop_na() %>%
  ggplot(aes(x = as.factor(resolution), y = avg_width)) +
  geom_boxplot() +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across All Cells")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_avg_width_boxplot.png"
))

res_tbl %>%
  drop_na() %>%
  ggplot(aes(x = as.factor(resolution), y = cluster_avg_widths)) +
  geom_boxplot() +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across Clusters")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_cluster_avg_width_boxplot.png"
))

ggplot(
  results_summary,
  aes(x = as.factor(resolution), y = mean_score, group = 1)
) +
  geom_errorbar(
    aes(
      ymin = mean_score - (1.96*standard_error_score),
      ymax = mean_score + (1.96*standard_error_score),
      width = .3
    ),
    color = "red"
  ) +
  geom_point(colour = "#619CFF") +
  geom_line(colour = "#619CFF") +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across All Cells")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_avg_width_95_percent_confidence_interval.png"
))

ggplot(
  results_summary,
  aes(x = as.factor(resolution), y = cluster_mean_score, group = 1) # scale should be the res range
) +
  geom_errorbar(
    aes(
      ymin = cluster_mean_score - (1.96*cluster_standard_error_score),
      ymax = cluster_mean_score + (1.96*cluster_standard_error_score),
      width = .3
    ),
    color = "red"
  ) +
  geom_point(colour = "#619CFF") +
  geom_line(colour = "#619CFF") +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across Clusters")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_cluster_avg_width_95_percent_confidence_interval.png"
))


writeLines(
  capture.output(sessionInfo()),
  file.path(opt$output, "sessionInfo.txt")
)
